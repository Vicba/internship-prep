{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f4a756b-ccc8-40d3-8be0-282fede9879d",
   "metadata": {},
   "source": [
    "Hit Rate is the fraction of queries for which the correct answer is present in the top-k retrievals. For a given query, if we have a top 'k' value of 5, and if the correct answer is present in the first 5 retrieved documents, then the hit rate will be 1; otherwise, it will be 0.\n",
    "\n",
    "Mean Reciprocal Rank (MRR) is based on the rank of the highest-placed relevant document. For a given query, we will get the rank of the relevant document and then compute the inverse of the rank to get the query score. For example, if the relevant document is ranked 1, then the score for the given query is 1; if the relevant document is ranked 2, then the score is 0.5 (1/2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "106853fd-9eb1-4681-9374-749de1120cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6f24dba-c20f-4054-9f50-45c36dd9e76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7bb607f-ef99-4d83-9dce-210e3bc56ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM\n",
    "from llama_index.llms import Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "481ab672-7f8f-46de-8830-57d6caa6c043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings\n",
    "from llama_index.embeddings import OpenAIEmbedding, HuggingFaceEmbedding, CohereEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b8a5ae9-92f7-47b5-81a3-0f2237cfc142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrievers\n",
    "from llama_index.retrievers import (\n",
    "    BaseRetriever,\n",
    "    VectorIndexRetriever,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c292c95-28cd-44d4-aa63-1280f4a2fd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rerankers\n",
    "from llama_index.indices.query.schema import QueryBundle, QueryType\n",
    "from llama_index.schema import NodeWithScore\n",
    "# from llama_index.indices.postprocessor.cohere_rerank import CohereRerank\n",
    "from llama_index.indices.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.finetuning.embeddings.common import EmbeddingQAFinetuneDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ee8a57c-50fa-41c6-8df1-b689c57dbfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluator\n",
    "from llama_index.evaluation import (\n",
    "    generate_question_context_pairs,\n",
    "    EmbeddingQAFinetuneDataset,\n",
    ")\n",
    "from llama_index.evaluation import RetrieverEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a34a7628-8c14-4adb-9174-94b0933d20d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import pandas as pd\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47ab443-e588-4fc2-9bbf-d99922d9c000",
   "metadata": {},
   "source": [
    "### load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3973762-34bd-4e07-a169-046ce433cf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"sciq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f914918a-0250-4cb8-ae3a-adad1bfc4c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the chunk size as 512 in node parser\n",
    "node_parser = SimpleNodeParser.from_defaults(chunk_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b647d75-2e66-4fbe-acb0-de001fe32f85",
   "metadata": {},
   "source": [
    "The dataset has more than 13K rows for simplicity take only 500 rows from them for our evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "902b2f12-32b8-4946-9b52-5c0049bafe96",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "filtered_queries = []\n",
    "counter = 0\n",
    "for train_row in dataset[\"train\"]:\n",
    "    # Remove the empty documents\n",
    "    if len(train_row[\"support\"].strip()) == 0:\n",
    "        continue\n",
    "    current_document = Document(text=train_row[\"support\"])\n",
    "    # If the number of nodes for the document is 1, use them for evaluation\n",
    "    if len(node_parser.get_nodes_from_documents([current_document])) == 1:\n",
    "        corpus.append(train_row[\"support\"])\n",
    "        filtered_queries.append(train_row[\"question\"])\n",
    "        counter += 1\n",
    "    # Limit to 500 documents\n",
    "    if counter == 500:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d4600f2-5b80-4b18-b443-9a58318db51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the nodes from documents\n",
    "documents = [Document(text=c) for c in corpus]\n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "# Manually assign node id for retrieval and evaluation\n",
    "for idx, node in enumerate(nodes):\n",
    "    node.id_ = f\"corpus_{idx}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9987dac8-068f-4cdc-897e-b1377d317a47",
   "metadata": {},
   "source": [
    "EmbeddingQAFinetuneDataset from LlamaIndex for our evaluation. EmbeddingQAFinetuneDataset needs queries dictionary, corpus dictionary, and the relevant mapping dictionary as inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2bb5642e-b60c-43bd-86b6-2f93aac93312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inputs for EmbeddingQAFinetuneDataset\n",
    "queries_dict = {f\"query_{index}\":filtered_queries[index] for index in range(counter)}\n",
    "corpus_dict = {f\"corpus_{index}\":corpus[index] for index in range(counter)}\n",
    "relevant_docs_dict = {f\"query_{index}\":[f\"corpus_{index}\"] for index in range(counter)}\n",
    "\n",
    "# Create QA dataset\n",
    "qa_dataset = EmbeddingQAFinetuneDataset(\n",
    "    queries=queries_dict,\n",
    "    corpus=corpus_dict,\n",
    "    relevant_docs=relevant_docs_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c22e16b-6483-475e-af31-9a83796480bc",
   "metadata": {},
   "source": [
    "embeddings and rerankers that we are going to evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4af20b2-428f-477e-b446-8b14852dda96",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDINGS = {\n",
    "    \"bge-large\": HuggingFaceEmbedding(model_name='BAAI/bge-large-en'),\n",
    "    \"JinaAI-Small\": HuggingFaceEmbedding(model_name='jinaai/jina-embeddings-v2-small-en', pooling='mean', trust_remote_code=True),\n",
    "    \"JinaAI-Base\": HuggingFaceEmbedding(model_name='jinaai/jina-embeddings-v2-base-en', pooling='mean', trust_remote_code=True),\n",
    "}\n",
    "\n",
    "RERANKERS = {\n",
    "    \"WithoutReranker\": \"None\",\n",
    "    \"bge-reranker-base\": SentenceTransformerRerank(model=\"BAAI/bge-reranker-base\", top_n=5),\n",
    "    \"bge-reranker-large\": SentenceTransformerRerank(model=\"BAAI/bge-reranker-large\", top_n=5)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613eba45-8144-45d3-b184-30049f1cca13",
   "metadata": {},
   "source": [
    "KERNEL DIES EVERYTIME HERE, (i have access to all embeddings and rerankers in HF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25d6bdf-c2eb-4097-870a-ef90b71a1fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "service_context = ServiceContext.from_defaults(llm=None, embed_model=embed_model)\n",
    "vector_index = VectorStoreIndex(nodes, service_context=service_context)\n",
    "vector_retriever = VectorIndexRetriever(index=vector_index, similarity_top_k=5, service_context=service_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef6514a-606d-44b8-98ca-559c685bf526",
   "metadata": {},
   "source": [
    "reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026a8fd2-405c-4787-9438-b0a69cbddb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Retriever\n",
    "class CustomRetriever(BaseRetriever):\n",
    "    \"\"\"Custom retriever that performs both Vector search and Reranking\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_retriever: VectorIndexRetriever,\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "\n",
    "        self._vector_retriever = vector_retriever\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve nodes given query.\"\"\"\n",
    "\n",
    "        retrieved_nodes = self._vector_retriever.retrieve(query_bundle)\n",
    "\n",
    "        if reranker != 'None':\n",
    "            retrieved_nodes = reranker.postprocess_nodes(retrieved_nodes, query_bundle)\n",
    "        else:\n",
    "            retrieved_nodes = retrieved_nodes[:5]\n",
    "\n",
    "        return retrieved_nodes\n",
    "\n",
    "    async def _aretrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Asynchronously retrieve nodes given query.\n",
    "\n",
    "        Implemented by the user.\n",
    "\n",
    "        \"\"\"\n",
    "        return self._retrieve(query_bundle)\n",
    "\n",
    "    async def aretrieve(self, str_or_query_bundle: QueryType) -> List[NodeWithScore]:\n",
    "        if isinstance(str_or_query_bundle, str):\n",
    "            str_or_query_bundle = QueryBundle(str_or_query_bundle)\n",
    "        return await self._aretrieve(str_or_query_bundle)\n",
    "\n",
    "custom_retriever = CustomRetriever(vector_retriever)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b63b12a-75c7-4fc3-88cd-7a8a5ff6733c",
   "metadata": {},
   "source": [
    "evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01248e9b-7cb0-4769-bc1d-f621a353efe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
    "    [\"mrr\", \"hit_rate\"], retriever=custom_retriever\n",
    ")\n",
    "eval_results = await retriever_evaluator.aevaluate_dataset(qa_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
